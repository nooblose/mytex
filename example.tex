\documentclass{article}

\usepackage{packages}

\usepackage{environments}

\usepackage{commands}

\title{Прикладная статистика в машинном обучении (2021-2022)}
\author{Лекции}
\date{}

\begin{document}

\maketitle

\section{Лекция №1}

\subsection{Энтропия}

Пусть $A$ -- событие.

\begin{definition}
    Назовём нежданностью (surprise) события $A$ величину
    \[
        s(A) = \log_{\frac 12} (A)
    \]

    \footnotesize{
        Смысл: $s(A)$ -- длина оптимального кодирования события $A$ в битах
    }
\end{definition}

Пусть $X$ -- дискретная случайная величина. 

\begin{definition}
    Энтропией назовём <<ожидаемую неожиданность>> от наблюдения $X$
    \[
        H(X) = \E(s_x), \quad s_t = \log_{\frac 12} P(X = t)
    \]

    \footnotesize{
        Смысл: сколько в среднем бит надо потратить на передачу одного значения $X$ при оптимальном кодировании

        Данетки: сколько в среднем нужно задать вопросов (с ответами да или нет), чтобы найти значение $X$
    }
\end{definition}

\begin{example}
    Рассмотрим следующую случайную величину

    \begin{center}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            $t$        & 5          & 6          & 7          & 8          \\ \hline
            $P(X = t)$ & $\frac 18$ & $\frac 18$ & $\frac 14$ & $\frac 12$ \\ \hline
        \end{tabular}
    \end{center}

    Тогда энтропией этой величины будет
    \[
        H(X) = \frac 18 \cdot 3 + \frac 18 \cdot 3 + \frac 14 \cdot 2 + \frac 12 \cdot 1 = 1.75 \text{ (бит)}
    \]

    Можно заметить, что функция $H(X)$ никак не зависит от значений случайной величины $X$, поэтому корректно писать
    \[
        H(X) = H(p), \quad p(t) = P(X = t) \text{ -- функция вероятностей}   
    \]
\end{example}

\subsection{Условная энтропия}

Пусть $X$ и $Y$ -- случайные величины.

\begin{definition}
    Условной энтропией назовём функцию
    \[
        H(X|Y) = \sum_y P(Y = y) \cdot H(X|Y = y),  
    \]
    где
    \[
        H(X|Y = y) = \sum_x P(X = x | Y = y) \cdot \log_{\frac 12} \left[ P(X = x|Y = y) \right]
    \]

    \footnotesize{
        Смысл: сколько в среднем нужно задать вопросов в данетке, чтобы узнать значение $X$, при условии, что знаем значение $Y$
    }
\end{definition}

\begin{example}
    Рассмотрим случайные величины $X$ и $Y$ с равновероятными исходами.

    \begin{center}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            $\omega$    & $a$ & $b$ & $c$  & $d$  \\ \hline
            $X(\omega)$ & $1$ & $1$ & $-1$ & $-1$ \\ \hline
            $Y(\omega)$ & $0$ & $5$ & $2$  & $2$  \\ \hline
        \end{tabular}
    \end{center}

    Заметим, что $H(X|Y) = 0$, поскольку зная значение $Y$ мы однозначно можем получить значение $X$.

    Теперь $H(Y|X)$: если $X = -1$, то мы сразу получаем $Y = 2$, иначе придётся задать один дополнительный вопрос (например, $x > 0$?), то есть
    \[
        H(Y|X) = \frac 12 \cdot 0 + \frac 12 \cdot 1 = \frac12
    \]

\end{example}

\subsection{Совместная энтропия}

Пусть $X$ и $Y$ -- случайные величины.

\begin{definition}
    Совместной энтропией случайных величин $X$ и $Y$ назовём функцию
    \[
        H(X, Y) = \sum_{x, y} P(X = x, Y = y) \cdot \log_{\frac12} P(X = x, Y = y)
    \]

    \footnotesize{
        Смысл: сколько в среднем нужно задать вопросов в данетке, чтобы узнать значения $X$ и $Y$
    }
\end{definition}

\begin{theorem}
    Для случайных величин $X$ и $Y$ выполняется, что
    \[
        H(X) + H(Y|X) = H(X, Y)  
    \]

    \footnotesize{
        Смысл: в среднем задаём $H(X)$ чтобы узнать $X$ и $H(Y|X)$ вопросов, чтобы узнать $Y$ при этом $X$. Таким образом мы узнаем и $X$ и $Y$.
    }
\end{theorem}

\begin{proof}
    TODO: раскрыть сумму и воспользоваться тем, что логарифм дроби равен разницы логарифмов.
\end{proof}

\subsection{Взаимная информация}

Пусть $X$ и $Y$ -- случайные величины.

\begin{definition}
    Взаимной (mutual) информацией назовём следующую функцию
    \[
        I(X, Y) = H(X) + H(Y) - H(X, Y)  
    \]

    \footnotesize{
        Смысл: сколько информации мы экономим, пытаясь сразу узнать значения $X$ и $Y$.
    }
\end{definition}

\subsection{Кросс-энтропия}

Пусть $p$ и $q$ -- некоторые распределения случайных дискретных величин.

\begin{definition}
    Кросс-энтропией из $p$ в $q$ назовём функцию
    \[
        CE(p\ ||\ q) = \E_p(\log_{\frac 12} q) = \sum_{\omega} p(\omega) \cdot \log_{\frac12} q(\omega)  
    \]

    \footnotesize{
        Смысл: сколько бит (в среднем) информации нужно получить, если истинное распределение $p$, а кодировка оптимальна под распределение $q$.
    }
\end{definition}

\begin{example}
    Пусть $p$ -- истинное распределение (его мы не знаем), а $q$ -- предполагаемое распределение.

    \begin{center}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            цвет         & Б          & Ф          & К          & З          \\ \hline
            $p$ (ист.)   & $\frac 12$ & $\frac 14$ & $\frac 18$ & $\frac 18$ \\ \hline
            $q$ (предп.) & $\frac 18$ & $\frac 18$ & $\frac 12$ & $\frac 14$ \\ \hline
        \end{tabular}
    \end{center}

    Поскольку мы используем неоптимальную кодировку, то на разгадывание значения потребуется в среднем больше вопросов.
\end{example}

\begin{remark}
    Пусть $p$ и $q$ -- распределения, тогда

    \begin{enumerate}
        \item $CE(p\ ||\ p) = H(p)$
        \item $CE(p\ ||\ q) \geqslant H(p)$
    \end{enumerate}
\end{remark}

\subsection{Дивергенция}

Пусть $p$ и $q$ -- распределения случайных величин.

\begin{definition}
    Дивергенцией Кульбана-Лябблера (Kullback-Leibler divergence) назовём функцию
    \[
        KL(p\ ||\ q) = CE(p\ ||\ q) - H(p) = CE(p\ ||\ q) - CE(p\ ||\ p)
    \]

    \footnotesize{
        Смысл: потери в битах от неоптимального кодирования.
    }
\end{definition}

\section{Лекция №2}

\subsection{Метод максимального правдоподобия}

Какая у нас стоит задача? Нам нужно прогнозировать, проверять гипотезы. У есть реальный мир (сложный) и есть модель реального мира с вектором параметров $\Theta$, а мы хотим получить $\hat \Theta$, то есть оценить параметры модели по реальным данным.

У нас будет два варианта обозначений:
\begin{enumerate}
    \item занудный;
    
    $\Theta_T$ -- истинное значение, $\Theta_H$ -- предполагаемое значение, $\hat \Theta$ -- оценка, $\Theta$ -- <<бегающий>> параметр.
    
    \item кратко;
    
    $\Theta$ -- истинное значение, $\hat \Theta$ -- оценка.
\end{enumerate}

Перед нами стоит задача нахождения универсального метода для оценки неизвестного параметра $\Theta_T$.

\medskip

\begin{example}
    Пусть мы прибыли в дикую африканскую местность и встретили племя, которому ничего не известно о науке. Мы замечаем, что у всех мужчин на руке синяя повязка, а у женщин -- жёлтая и у нас есть две гипотезы, объясняющие то, что мы видим:
    \begin{itemize}
        \item $H_*$ -- каждый день мужчины носят синюю повязку, а женщины -- жёлтую;
        \item $H_+$ -- сегодня праздник повязок, который проходит раз в 10 лет;
    \end{itemize}

    Заметим, что
    \[  
        P\left( \text{данные}\ |\ H_* \right) = 1 \quad P\left( \text{данные}\ |\ H_+ \right) = \frac1{3650}
    \]

    Метод максимального правдоподобия говорит, выбирайте ту гипотезу, при которой вероятность того, что вы видите, максимальна. То есть в нашем случае гипотеза $H_*$ выглядит более правдоподобной.
\end{example}

\begin{example}
    Пусть мы ловим ёжиков (кормим их и отпускаем).

    \begin{center}
        \begin{tabular}{|c|c|c|c|}
            \hline
            $x$ (вес)    & $2$      & $3$       & $4$            \\ \hline
            $P(X_i = x)$ & $\alpha$ & $2\alpha$ & $1 - 3\alpha$ \\ \hline
        \end{tabular}, \quad $X_i$ -- независимые одинаково распределённые
    \end{center}

    Пусть мы поймали ежей с весами 3, 4, 3, 3, 3, 4, 2 и перед нами стоит задача найти $\hat \alpha$ методом максимального правдоподобия. То есть нам нужно посчитать
    \[
        L(\alpha) = P\left( X_1=3, X_2=4, X_3=3, X_4=3, X_5=3, X_6=4, X_7=2\ |\ \alpha \right)
    \]

    Эта функция называется \textit{функцией правдоподобия} (likelihood function). Поскольку события у нас независимые, то получаем
    \[
        L(\alpha) = (2\alpha) \cdot (1 - 3\alpha) \cdot (2\alpha) \cdot (2\alpha) \cdot (2\alpha) \cdot (1 - 3\alpha) \cdot \alpha
    \]

    Чтобы было ближе к энтропии, возьмём логарифм от функции правдоподобия (\textit{логарифмическая функция правдоподобия})
    \[
        \ell(\alpha) = \ln L(\alpha)  
    \]

    Соответственно сам \textit{метод максимального правдоподобия} выглядит следующим образом
    \[
        \hat \alpha = \max_\alpha \ell(\alpha)
    \]

    В нашем случае
    \[
        \ell(\alpha) = \ln \left[ (2\alpha)^4 \cdot (1 - 3\alpha)^2 \cdot \alpha \right] = 4\ln 2 + 5\ln\alpha + 2\ln(1 - 3\alpha)
    \]

    Остаётся взять производную, приравнять к нулю и получить значение $\hat \alpha$.
\end{example}

\begin{example}
    Пусть мы каждый день едем от дома к метро на автобусе. Естественно, перед тем как сесть в автобус, мы его некоторое время ожидаем. Пусть $X_i$ -- время ожидания автобуса в $i$-ый день и $X_i \sim \exp(\lambda)$ (распределение выбрали экспоненциальное, поскольку 1) значение всегда неотрицательное; 2) не принимает большие значения) и $X_i$ независимые.

    Вспомним, что функция распределения $\exp(\lambda)$ выглядит следующим образом
    \[
        f(x) = 
        \begin{cases}
            \lambda \cdot e^{-\lambda x}, & \text{при $x \geqslant 0$} \\
            0 & \text{иначе}
        \end{cases}  
    \]

    а) Пусть $X_1 = 5$, $X_2 = 10$, $X_3 = 4$ и нам нужно найти оценку $\hat\lambda_{\text{ML}}$.

    Тут функцией правдоподобия будет функция плотности для $X_1, X_2$ и $X_3$
    \[
        L(\lambda) = f(x_1, x_2, x_3\ |\ \lambda) = \lambda \cdot e^{-\lambda \cdot 5} \cdot \lambda \cdot e^{-\lambda \cdot 10} \lambda \cdot e^{-\lambda \cdot 4}
    \]

    Теперь получим из этого логарифмическую функцию правдоподобия.
    \[
        \ell(\lambda) = 3\ln \lambda - 19\lambda
    \]
    \[
        \ell'(\lambda) = \frac3\lambda - 19
    \]

    Приравниваем к нулю и получаем оценку
    \[
        \hat\lambda = \frac3{19}  
    \]

    Ну а в общем случае у нас будет аналогичная оценка
    \[
        \ell(\lambda) = n\ln \lambda - \sum X_i \cdot \lambda  
    \]
    \[
        \hat \lambda = \frac{n}{\sum X_i}  
    \]
\end{example}

\subsubsection{Хорошие свойства метода максимального правдоподобия}

\begin{enumerate}
    \item Привязка к энтропии.
    
    Пусть нам нужно отгадать не одно значение случайной величины $X$, а $n$ значений. Вспомним формулу кросс-энтропии
    \[
        CE(f_{\Theta_{T}}\ ||\ f_{\Theta}) = -E_{\Theta_T} \left[ \ln f_{\Theta} (X_1, \ldots, X_n) \right]  
    \]

    Отсюда заметим, что
    \[
        -CE = \E_{\Theta_T} \left[ \ell(x | \Theta) \right]
    \]

    То есть на самом деле минус кросс-энтропия выполняет роль матожидания от функции правдоподобия и, поэтому, уже интуитивно понятно, что функция правдоподобия в среднем в каком-то смысле неплохо попадает в нужные значения.
\end{enumerate}

\end{document}
